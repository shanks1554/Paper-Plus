{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1154e33f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c30c7f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bd92f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Deep\\OneDrive\\Desktop\\Projects\\Paper Plus\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_classic.chains import RetrievalQA\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1390825e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Deep\\AppData\\Local\\Temp\\ipykernel_8028\\118603400.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e139c1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.load_local(\n",
    "    \"../indexes/artificial_intelligence\",\n",
    "    embedding_model,\n",
    "    allow_dangerous_deserialization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d3f591",
   "metadata": {},
   "source": [
    "## Custom grounded prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c6f2bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_PROMPT_TEMPLATE = \"\"\"\n",
    "Use the pieces of information provided in the context to answer the user's question.\n",
    "If you do not know the answer, say that you do not know.\n",
    "Do not make up an answer.\n",
    "Do not use information outside the given context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer directly. No small talk.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "558c5505",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=CUSTOM_PROMPT_TEMPLATE,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6edcb8",
   "metadata": {},
   "source": [
    "## Hugging Face InferenceClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbc655e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = InferenceClient(\n",
    "    model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    token=os.getenv(\"HF_TOKEN\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "087f7dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hf_llm_call(prompt: str, **kwargs) -> str:\n",
    "    # Convert LangChain prompt object to string\n",
    "    if hasattr(prompt, \"to_string\"):\n",
    "        prompt = prompt.to_string()\n",
    "    \n",
    "    response = client.chat_completion(\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens = 512,\n",
    "        temperature = 0.2\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a2e1495",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = RunnableLambda(hf_llm_call)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03428d7",
   "metadata": {},
   "source": [
    "## Create the RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "303d45ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm = llm,\n",
    "    chain_type = \"stuff\",\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": 16}),\n",
    "    return_source_documents = True,\n",
    "    chain_type_kwargs = {\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a80a1886",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the recent research trends in Artificial Intelligence?\"\n",
    "\n",
    "response = qa_chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ca86d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER:\n",
      " The recent research trends in Artificial Intelligence are indicated by the dramatic increase in the number of publications on AI in information engineering from 2014 to 2024, as shown in Figure 1. The number of papers increased from 7 in 2014 to 200 in 2024, a nearly 20-fold increase, with an explosive growth phase in 2019-2024 and a deepening research focus.\n"
     ]
    }
   ],
   "source": [
    "print(\"ANSWER:\\n\", response[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab41df95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SOURCES:\n",
      "AI computer science 4.pdf\n",
      "AI computer science 1.pdf\n",
      "AI computer science 1.pdf\n",
      "AI computer science 4.pdf\n",
      "3727353.3727478.pdf\n",
      "AI computer science 5.pdf\n",
      "AI computer science 5.pdf\n",
      "AI computer science 5.pdf\n",
      "AI computer science 5.pdf\n",
      "AI computer science 5.pdf\n",
      "AI computer science 5.pdf\n",
      "AI computer science 4.pdf\n",
      "AI computer science 4.pdf\n",
      "AI computer science 5.pdf\n",
      "AI computer science 5.pdf\n",
      "AI computer science 5.pdf\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSOURCES:\")\n",
    "for doc in response[\"source_documents\"]:\n",
    "    print(doc.metadata.get(\"source\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
